{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import argparse\n",
    "import clean\n",
    "import lib\n",
    "import time\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch as t\n",
    "import os\n",
    "import models\n",
    "# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "arg_corpus = \"small\"\n",
    "arg_vector = \"fasttext\"\n",
    "arg_units = 256\n",
    "arg_train_emb = True\n",
    "arg_run = 1\n",
    "arg_batch_size = 16\n",
    "\n",
    "corpus = 'data/posdata/' + arg_corpus\n",
    "if arg_vector == \"scratch\":\n",
    "    vector_file_name = None\n",
    "    vec_name = \"scratch\"\n",
    "else:\n",
    "    vec_name = arg_vector\n",
    "    vector_file_name = 'vectors/' + arg_vector + \".vec\"\n",
    "\n",
    "accuracy_file = \"result/pos/{0}-{1}-{2}-{3}-{4}\".format(arg_corpus, vec_name, arg_units, arg_train_emb, arg_run)\n",
    "hidden_size = arg_units\n",
    "epochs = 10\n",
    "batch_size = 16\n",
    "max_seq_len = 64\n",
    "train_embedding = arg_train_emb == 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "seed = 12432\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "t.manual_seed(seed)\n",
    "\n",
    "start_of_sentense = '<sos>'\n",
    "end_of_sentense = '<eos>'\n",
    "start_of_tag = '<sot>'\n",
    "end_of_tag = '<eot>'\n",
    "pad_word = '<pad>'\n",
    "pad_tag = '<pad>'\n",
    "start_pad_char = 't'\n",
    "stop_pad_char = 'p'\n",
    "end_pad_char = 'd'\n",
    "char_pad_char = 'c'\n",
    "unk_char = 'n'\n",
    "\n",
    "train_ratio = .7\n",
    "test_batch_size = batch_size\n",
    "k_fold = 10\n",
    "max_char_length = 14\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pos_data, pos_vocabs, tags = lib.get_pos_data_v2(corpus)\n",
    "# pos_data = pos_data[:2000]\n",
    "p = []\n",
    "for line in pos_data:\n",
    "    if len(line) < max_seq_len:\n",
    "        p.append(line)\n",
    "pos_data = p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "37 ['<eot>', '<pad>', '<sot>', 'ADJ', 'ADJC', 'ADJP', 'ADJPC', 'ADV', 'AUX', 'CONJ', 'INT', 'N', 'NC', 'NP', 'NPC', 'NUMC', 'NUMCR', 'NUMOR', 'NUMP', 'NUMPC', 'PREP', 'PREPC', 'PRON', 'PRONC', 'PRONP', 'PRONPC', 'PRREP', 'PUNC', 'PUNCP', 'PUNCUMCR', 'UNC', 'V', 'VC', 'VN', 'VP', 'VPC', 'VREL']\n"
    }
   ],
   "source": [
    "\n",
    "charset = list(open('charset.txt', encoding='utf-8').read()) + [start_pad_char, stop_pad_char, end_pad_char, unk_char, char_pad_char]\n",
    "charset.pop(charset.index(' '))\n",
    "charset.pop(charset.index('\\n'))\n",
    "# charset.pop(charset.index(''))\n",
    "clean_data, clean_pos_vocabs, total_words = pos_data, pos_vocabs, tags\n",
    "word2feat, word_feat_length = lib.get_word_feats(\"analysis.txt\")\n",
    "word2feat[start_of_sentense] = [0]*word_feat_length\n",
    "clean_pos_vocabs.extend([start_of_sentense, end_of_sentense, pad_word])\n",
    "tags.extend([start_of_tag, end_of_tag, pad_tag])\n",
    "word2id = {word: i for i, word in enumerate(clean_pos_vocabs)}\n",
    "id2word = {i: word for i, word in enumerate(clean_pos_vocabs)}\n",
    "tag2id = {tag: tid for tid, tag in enumerate(tags)}\n",
    "id2tag = {tid: tag for tid, tag in enumerate(tags)}\n",
    "char2id = {c:i for i, c in enumerate(charset)}\n",
    "id2char = {i:c for i, c in enumerate(charset)}\n",
    "print(len(id2tag), sorted(id2tag.values()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def try_get_word_vector(allw2v, word, unk_vector):\n",
    "    if '-' in word:\n",
    "        word = word.replace('-', ' ')\n",
    "    word = clean.clean_to_text(word)\n",
    "    words = word.split(' ')\n",
    "    v = []\n",
    "    for c in words:\n",
    "        if c in allw2v:\n",
    "            v.append(allw2v[c])\n",
    "        else:\n",
    "            v.append(unk_vector)\n",
    "\n",
    "    v = np.mean(v, axis=0)\n",
    "    return v, word\n",
    "\n",
    "def get_word_vectors(filename):\n",
    "    word2vec = {}\n",
    "    word2clean = {}\n",
    "    allw2v = {}\n",
    "    with open(filename, encoding='utf-8') as f:\n",
    "        line = f.readline().strip().split()\n",
    "        vocab_size, embed_size = int(line[0]), int(line[1])\n",
    "        # embed_size = embed_size + word_feat_length\n",
    "        for line in f:\n",
    "            line = line.strip().split()\n",
    "            word, vec = line[0], [float(x) for x in line[1:]]\n",
    "            word = word.strip()\n",
    "            allw2v[word] = np.array(vec)\n",
    "    unk_vector = allw2v['</s>']\n",
    "\n",
    "    for word in word2id.keys():\n",
    "        if word in allw2v:\n",
    "            word2clean[word] = word\n",
    "            word2vec[word] = allw2v[word]\n",
    "            # if word in word2feat:\n",
    "            #     feat = word2feat[word]\n",
    "            # else:\n",
    "            #     feat = [0]*word_feat_length\n",
    "            # word2vec[word] = np.concatenate((allw2v[word], feat), axis=0)\n",
    "        else:\n",
    "            vec, new_word = try_get_word_vector(allw2v, word, unk_vector)\n",
    "            word2clean[word] = new_word\n",
    "            word2vec[new_word] = vec\n",
    "            # if new_word in word2feat:\n",
    "            #     feat = word2feat[new_word]\n",
    "            # else:\n",
    "            #     feat = [0]*word_feat_length\n",
    "            # word2vec[new_word] = np.concatenate((vec, feat), axis=0)\n",
    "        \n",
    "\n",
    "    vectors = np.empty((len(id2word), embed_size))\n",
    "    for word_id in id2word.keys():\n",
    "        vec = word2vec[word2clean[id2word[word_id]]]\n",
    "        vectors[word_id] = vec\n",
    "\n",
    "    one_vec = np.ones(embed_size)\n",
    "    one_vec = one_vec/np.linalg.norm(one_vec)\n",
    "    vectors[word2id[start_of_sentense]] = one_vec\n",
    "    vectors[word2id[end_of_sentense]] = -one_vec\n",
    "    vectors[word2id[pad_word]] = np.zeros(embed_size)\n",
    "    return vectors, word2vec, word2clean, embed_size, unk_vector\n",
    "\n",
    "def word_to_ids(word, char2id, max_len=13):\n",
    "    idx = -1\n",
    "    chars = []\n",
    "    for c in word:\n",
    "        if c in char2id:\n",
    "            idx = char2id[c]\n",
    "        else:\n",
    "            idx = char2id[unk_char]\n",
    "        chars.append(idx)\n",
    "    chars = chars + [char2id[char_pad_char]] * (max_len - len(chars))\n",
    "    return chars\n",
    "\n",
    "\n",
    "def prepare_line_data(line,  max_seq_length, max_char_length):\n",
    "    x, y, f = [], [], []\n",
    "    char_x = []\n",
    "    for [word, tag] in line:\n",
    "        word_id = word2id[word]\n",
    "        tag_id = tag2id[tag]\n",
    "        x.append(word_id)\n",
    "        y.append(tag_id)\n",
    "        if word not in word2feat:\n",
    "            vec =  [0]*word_feat_length\n",
    "        else:\n",
    "            vec = word2feat[word]\n",
    "        f.append(vec)\n",
    "        word = clean.clean_to_text(word)\n",
    "        char_x.append(word_to_ids(word, char2id, max_char_length))\n",
    "    \n",
    "    x = [word2id[start_of_sentense]] + x + [word2id[end_of_sentense]]\n",
    "    y = [tag2id[start_of_tag]] + y + [tag2id[end_of_tag]]\n",
    "    f = [word2feat[start_of_sentense]] + f + [word2feat[start_of_sentense]]\n",
    "    \n",
    "    start_pad_char_word = [char2id[start_pad_char]]*max_char_length\n",
    "    stop_pad_char_word = [char2id[stop_pad_char]]*max_char_length\n",
    "    end_pad_char_word = [char2id[end_pad_char]]*max_char_length\n",
    "    char_x = [start_pad_char_word] + char_x + [stop_pad_char_word]\n",
    "\n",
    "    pad_len = max_seq_length - len(x)\n",
    "    xpad = [word2id[pad_word]] * pad_len\n",
    "    ypad = [tag2id[pad_tag]] * pad_len\n",
    "    \n",
    "    fpad = [word2feat[start_of_sentense]] * pad_len\n",
    "    c_pad = [end_pad_char_word] * pad_len\n",
    "    mask = [1]*len(x) + [0] * pad_len\n",
    "    x = x + xpad\n",
    "    y = y + ypad\n",
    "    f = f + fpad\n",
    "    \n",
    "    c = char_x + c_pad\n",
    "    return x, y, c, f, mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(9477, 66, 29) (9477, 66)\nLoading unks & knows\n"
    }
   ],
   "source": [
    "\n",
    "X, Y, C, F, M = [], [], [], [], []\n",
    "for line in clean_data:\n",
    "    x, y, c, f, m = prepare_line_data(line, max_seq_len + 2, max_char_length)\n",
    "    X.append(x)\n",
    "    Y.append(y)\n",
    "    C.append(c)\n",
    "    M.append(m)\n",
    "    F.append(f)\n",
    "X = np.array(X)\n",
    "Y = np.array(Y)\n",
    "M = np.array(M)\n",
    "F = np.array(F)\n",
    "print(F.shape, X.shape)\n",
    "C = np.array(C)\n",
    "even_len = len(X) - len(X) % k_fold\n",
    "indexes = np.arange(even_len)\n",
    "np.random.shuffle(indexes)\n",
    "X = X[indexes]\n",
    "Y = Y[indexes]\n",
    "M = M[indexes]\n",
    "C = C[indexes]\n",
    "F = F[indexes]\n",
    "\n",
    "fold_size = even_len // k_fold\n",
    "folds = {}\n",
    "for i in range(k_fold):\n",
    "    start = i * fold_size\n",
    "    end = (i+1) * fold_size\n",
    "    fold_indexes = indexes[start:end]\n",
    "    fold_x = X[fold_indexes]\n",
    "    fold_y = Y[fold_indexes]\n",
    "    fold_m = M[fold_indexes]\n",
    "    fold_c = C[fold_indexes]\n",
    "    fold_f = F[fold_indexes]\n",
    "    folds[i] = (fold_x, fold_y, fold_c, fold_f, fold_m)\n",
    "\n",
    "vectors = None\n",
    "embed_size = 200 #+ word_feat_length\n",
    "if vector_file_name is not None:\n",
    "    vectors, word2vec, word2clean, embed_size, unk_vector = get_word_vectors(vector_file_name)\n",
    "# vectors2, _, _, _ = get_word_vectors(\"vectors/fasttext-alpha.vec\")\n",
    "# with open(\"vocab.txt\", encoding='utf-8', mode='w') as f:\n",
    "#     for word in set(word2clean.values()):\n",
    "#         f.write(word)\n",
    "#         f.write(' ')\n",
    "# print(\"finish\")\n",
    "print(\"Loading unks & knows\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_unknown_words(train_x, test_x):\n",
    "    knowns = set([])\n",
    "    unknowns = set([])\n",
    "    test_knowns = set([])\n",
    "    for line in train_x:\n",
    "        for x in line:\n",
    "            knowns.add(x)\n",
    "    mat = []\n",
    "    for line in test_x:\n",
    "        row = []\n",
    "        for x in line:\n",
    "            if x not in knowns:\n",
    "                unknowns.add(x)\n",
    "                row.append(1)\n",
    "            else:\n",
    "                test_knowns.add(x)\n",
    "                row.append(0)\n",
    "        mat.append(row)\n",
    "    mat = np.array(mat, np.float32)\n",
    "    return knowns, unknowns, test_knowns, mat\n",
    "\n",
    "def generate(data, batch_size):\n",
    "    train_x, train_y, train_c, train_f,  train_m = data\n",
    "    current = 0\n",
    "    n_batches = len(train_x) // batch_size\n",
    "    indexes = np.arange(len(train_x))\n",
    "    np.random.shuffle(indexes)\n",
    "    while True:\n",
    "        bs = indexes[current:current+batch_size]\n",
    "        x = train_x[bs]\n",
    "        y = train_y[bs]\n",
    "        m = train_m[bs]\n",
    "        c = train_c[bs]\n",
    "        f = train_f[bs]\n",
    "        yield x, y, c, f, m\n",
    "        current += batch_size\n",
    "        if current >= n_batches * batch_size:\n",
    "            current = 0\n",
    "            np.random.shuffle(indexes)\n",
    "\n",
    "def evaluate(targtes, preds):\n",
    "    all_ys = targtes\n",
    "    all_preds = preds\n",
    "    precision = precision_score(all_ys, all_preds, average=None)\n",
    "    recall = recall_score(all_ys, all_preds, average=None)\n",
    "    f1 = f1_score(all_ys, all_preds, average=None)\n",
    "    accuracy = accuracy_score(all_ys, all_preds)\n",
    "    return np.mean(accuracy), np.mean(precision), np.mean(recall), np.mean(f1)\n",
    "\n",
    "def test_model(model, test_data, batch_size, unknowns):\n",
    "    test_n_batches = len(test_data[0]) // batch_size\n",
    "    test_gen = generate(test_data, batch_size)\n",
    "    allo = []\n",
    "    unks = []\n",
    "    all_print = []\n",
    "    for i in range(test_n_batches):\n",
    "        x, y, c, f, m = next(test_gen)\n",
    "        xx = t.tensor(x, dtype=t.long).cuda()\n",
    "        # mm = t.tensor(m, dtype=t.long).cuda()\n",
    "        f = t.tensor(f, dtype=t.float32).cuda()\n",
    "        c = t.tensor(c, dtype=t.long).cuda()\n",
    "        z = model(xx, c, f)\n",
    "        preds = t.argmax(z, dim=2).detach().cpu().numpy()\n",
    "        for j in range(len(preds)):\n",
    "            k = np.argwhere(y[j] == tag2id[end_of_tag])[0][0]\n",
    "            pred_row = preds[j]\n",
    "            for uk  in range(k):\n",
    "                py = y[j][uk]\n",
    "                pp = pred_row[uk]\n",
    "                px = x[j][uk]\n",
    "                # print([px, py, pp])\n",
    "                if px in unknowns:\n",
    "                    allo.append([px, py, pp])\n",
    "                    if py != pp:\n",
    "                        line  = \"{0} {1} {2}\\n\".format(id2word[px], id2tag[py], id2tag[pp])\n",
    "                        all_print.append(line)\n",
    "                else:\n",
    "                    unks.append([px, py, pp])\n",
    "    allo = np.array(allo) \n",
    "    unks = np.array(unks)\n",
    "    every = np.concatenate((allo, unks), axis=0)\n",
    "    ek = evaluate(allo[:, 1], allo[:, 2])\n",
    "    eu = evaluate(unks[:, 1], unks[:, 2])\n",
    "    ee = evaluate(every[:, 1], every[:, 2])\n",
    "    open(\"checks\", encoding='utf-8', mode='w').writelines(all_print)\n",
    "    return [ek, eu, ee]\n",
    "\n",
    "def train_model(train, test_data, batch_size, epochs, n_batches, unknowns):\n",
    "    gen = generate(train, batch_size)\n",
    "    model = models.BiLSTMChar(len(word2id), embed_size, hidden_size, len(tag2id), len(char2id), 64, max_char_length, vectors, train_embedding=train_embedding)\n",
    "    model.init_weights()\n",
    "    model.cuda()\n",
    "    # print(model)\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    optimizer = t.optim.Adamax(model.parameters(), lr=0.001)\n",
    "    accs = []\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for batch in range(n_batches):\n",
    "\n",
    "            x, y, c, f, m = next(gen)\n",
    "            x = t.tensor(x, dtype=t.long).cuda()\n",
    "            y = t.tensor(y, dtype=t.long).cuda()\n",
    "            f = t.tensor(f, dtype=t.float32).cuda()\n",
    "            c = t.tensor(c, dtype=t.long).cuda()\n",
    "            model.zero_grad()\n",
    "            z = model(x, c, f)\n",
    "            z = z.view(-1, len(tag2id))\n",
    "            y = y.view(-1)\n",
    "            loss = loss_function(z, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            batch_loss = loss.detach().cpu().numpy()\n",
    "            total_loss += batch_loss\n",
    "        accuracy = test_model(model, test_data, test_batch_size, unknowns)\n",
    "        accs.append(accuracy)\n",
    "        loss = total_loss / n_batches\n",
    "        print(\"Epoch: {0}, Loss: {1:.3}, Test: {2:.3}, {3:.3}, {4:.3}\".format(epoch, loss, accuracy[0][0], accuracy[1][0], accuracy[2][0]))\n",
    "        accuracy.insert(0, [loss])\n",
    "    return accs\n",
    "\n",
    "def save_acc(accs, fold):\n",
    "    f = open(accuracy_file + \"-\" + str(fold), mode='w')\n",
    "    for acc in accs:\n",
    "        line = []\n",
    "        for p in acc:\n",
    "            line.extend(p)\n",
    "        line = \",\".join([str(x) for x in line])\n",
    "        f.write(line)\n",
    "        f.write('\\n')\n",
    "    f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Starting training\nFold 0/10\n"
    },
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "cuDNN error: CUDNN_STATUS_MAPPING_ERROR",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-3168295e58c5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[0mtrain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_c\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_f\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_m\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[0mtest_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_c\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_f\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_m\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m     \u001b[0maccss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_batches\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munknowns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m     \u001b[0msave_acc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maccss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfold\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-17-b30ef1ed03be>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(train, test_data, batch_size, epochs, n_batches, unknowns)\u001b[0m\n\u001b[0;32m    105\u001b[0m             \u001b[0mc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 107\u001b[1;33m             \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    108\u001b[0m             \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mz\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtag2id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m             \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tf2\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    551\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Library\\Research\\Extrinsic\\models.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x, cs, f)\u001b[0m\n\u001b[0;32m    185\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    186\u001b[0m             \u001b[0mc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchar_embedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 187\u001b[1;33m             \u001b[0mc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcnn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    188\u001b[0m             \u001b[0mc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m             \u001b[0mc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tf2\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    551\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tf2\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    347\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    348\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 349\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    350\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    351\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tf2\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[1;34m(self, input, weight)\u001b[0m\n\u001b[0;32m    344\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m    345\u001b[0m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[1;32m--> 346\u001b[1;33m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[0;32m    347\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    348\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: cuDNN error: CUDNN_STATUS_MAPPING_ERROR"
     ]
    }
   ],
   "source": [
    "\n",
    "fold_unks ={}\n",
    "for i in range(k_fold):\n",
    "    test_x, test_y, test_c, test_f, test_m = folds[i]\n",
    "    train_x, train_y, train_m = [], [], []\n",
    "    for k in range(k_fold):\n",
    "        if k != i:\n",
    "            x, y, c, f, m = folds[k]\n",
    "            train_x.append(x)\n",
    "    train_x = np.vstack(train_x)\n",
    "    knowns, unknowns, test_knowns, unk_mask = get_unknown_words(train_x, test_x)\n",
    "    fold_unks[i] = (knowns, unknowns, test_knowns, unk_mask)\n",
    "\n",
    "print(\"Starting training\")\n",
    "\n",
    "for fold in range(k_fold):\n",
    "    test_x, test_y, test_c, test_f, test_m = folds[fold]\n",
    "    train_x, train_y, train_c, train_f, train_m = [], [], [], [], []\n",
    "    for k in range(k_fold):\n",
    "        if k != fold:\n",
    "            x, y, c, f, m = folds[k]\n",
    "            train_x.append(x)\n",
    "            train_y.append(y)\n",
    "            train_m.append(m)\n",
    "            train_c.append(c)\n",
    "            train_f.append(f)\n",
    "    train_x = np.vstack(train_x)\n",
    "    train_y = np.vstack(train_y)\n",
    "    train_m = np.vstack(train_m)\n",
    "    train_c = np.vstack(train_c)\n",
    "    train_f = np.vstack(train_f)\n",
    "    n_batches = len(train_x) // batch_size\n",
    "    (knowns, unknowns, test_knowns, mask) = fold_unks[fold]\n",
    "    print(\"Fold {0}/10\".format(fold))\n",
    "    train = (train_x, train_y, train_c, train_f, train_m)\n",
    "    test_data = (test_x, test_y, test_c, test_f, test_m)\n",
    "    accss = train_model(train, test_data, batch_size, epochs, n_batches, unknowns)\n",
    "    save_acc(accss, fold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}